{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unity BYOC Tutorial\n",
    "\n",
    "  Bring-Your-Own-Codegen (BYOC) is the interface that TVM offers to enable integration of external libraries like TensorRT, Cutlass, DNNL, etc.  This doc aims to provide high-level idea about how to use BYOC in TVM Unity in a composable and modular way."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-level Guide\n",
    "\n",
    "### Setup\n",
    "\n",
    "  Build TVM with your BYOC in `config.cmake`. \n",
    "  For example, if you want to use TensorRT:\n",
    "\n",
    "```python\n",
    "set(USE_TENSORRT_CODEGEN ON)\n",
    "set(USE_TENSORRT_RUNTIME ON)\n",
    "```\n",
    "\n",
    "### Basic workflow\n",
    "\n",
    "Unity BYOC offers a pattern-based offloading mechanism: users define a set of operator patterns they want to run with their library of interest and apply a pass sequence to perform the offloading. The following example showcases the end-to-end workflow. \n",
    "\n",
    "(1) Prepare a model you want to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spark/.local/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "import tvm\n",
    "from tvm import relax\n",
    "from tvm.script.parser import relax as R\n",
    "import numpy as np\n",
    "\n",
    "# Define an example IRModule\n",
    "@tvm.script.ir_module\n",
    "class InputModule:\n",
    "    @R.function\n",
    "    def main(\n",
    "        x: R.Tensor((16, 16), \"float32\"), y: R.Tensor((16, 16), \"float32\")\n",
    "    ) -> R.Tensor((16, 16), \"float32\"):\n",
    "        with R.dataflow():\n",
    "            z1 = R.multiply(x, y)\n",
    "            z2 = R.add(z1, x)\n",
    "            z3 = R.add(z1, z2)\n",
    "            z4 = R.multiply(z3, z2)\n",
    "            z5 = R.add(z4, z1)\n",
    "            R.output(z5)\n",
    "        return z5\n",
    "\n",
    "\n",
    "mod = InputModule"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) Define a list of operator patterns you want to match and execute with TensorRT. Let's try to match every `multiply` and `add` operator to pass the whole graph in this example. We define each pattern in Relax pattern language and assign them names in \"${LIBRARY_NAME}.{$PATTERN_NAME}\" notation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tvm.relax.dpl import is_op, wildcard\n",
    "patterns = [\n",
    "        (\"tensorrt.multiply\", is_op(\"relax.multiply\")(wildcard(), wildcard())),\n",
    "        (\"tensorrt.add\", is_op(\"relax.add\")(wildcard(), wildcard())),\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although this example uses simple patterns, pattern can be more complicated if necessary. For instance, you may want to match when inputs have certain shapes or a certain sequene of operators. To learn more about Relax pattern language, please see [this reference](https://github.com/tlc-pack/relax/issues/160).\n",
    "\n",
    "(3) Run a series of of following passes: \n",
    "* `FuseOpsByPattern` \n",
    "* `MergeCompositeFunctions`\n",
    "* `RunCodegen`\n",
    "\n",
    "In practice, you will run them consequtively within `tvm.transform.Sequential`. To demonstrate what each pass does, this example will apply them one-by-one and walkthrough the changes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #007979; font-style: italic\"># from tvm.script import ir as I</span>\n",
       "<span style=\"color: #007979; font-style: italic\"># from tvm.script import relax as R</span>\n",
       "\n",
       "\n",
       "<span style=\"color: #AA22FF\">@I</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>ir_module\n",
       "<span style=\"color: #008000; font-weight: bold\">class</span> <span style=\"color: #0000FF; font-weight: bold\">Module</span>:\n",
       "    <span style=\"color: #AA22FF\">@R</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>function\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">fused_relax_add</span>(\n",
       "        lv: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), x: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)\n",
       "    ) <span style=\"color: #AA22FF; font-weight: bold\">-&gt;</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>):\n",
       "        R<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;Composite&quot;</span>: <span style=\"color: #BA2121\">&quot;tensorrt.add&quot;</span>, <span style=\"color: #BA2121\">&quot;Primitive&quot;</span>: <span style=\"color: #008000\">1</span>})\n",
       "        <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>dataflow():\n",
       "            gv: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>add(lv, x)\n",
       "            R<span style=\"color: #AA22FF; font-weight: bold\">.</span>output(gv)\n",
       "        <span style=\"color: #008000; font-weight: bold\">return</span> gv\n",
       "\n",
       "    <span style=\"color: #AA22FF\">@R</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>function\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">fused_relax_multiply</span>(\n",
       "        x: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), y: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)\n",
       "    ) <span style=\"color: #AA22FF; font-weight: bold\">-&gt;</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>):\n",
       "        R<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;Composite&quot;</span>: <span style=\"color: #BA2121\">&quot;tensorrt.multiply&quot;</span>, <span style=\"color: #BA2121\">&quot;Primitive&quot;</span>: <span style=\"color: #008000\">1</span>})\n",
       "        <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>dataflow():\n",
       "            gv: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>multiply(x, y)\n",
       "            R<span style=\"color: #AA22FF; font-weight: bold\">.</span>output(gv)\n",
       "        <span style=\"color: #008000; font-weight: bold\">return</span> gv\n",
       "\n",
       "    <span style=\"color: #AA22FF\">@R</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>function\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">main</span>(\n",
       "        x: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), y: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)\n",
       "    ) <span style=\"color: #AA22FF; font-weight: bold\">-&gt;</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>):\n",
       "        cls <span style=\"color: #AA22FF; font-weight: bold\">=</span> Module\n",
       "        <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>dataflow():\n",
       "            lv: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> cls<span style=\"color: #AA22FF; font-weight: bold\">.</span>fused_relax_multiply(x, y)\n",
       "            lv_1: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> cls<span style=\"color: #AA22FF; font-weight: bold\">.</span>fused_relax_add(lv, x)\n",
       "            lv1: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> cls<span style=\"color: #AA22FF; font-weight: bold\">.</span>fused_relax_add(lv, lv_1)\n",
       "            lv1_1: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> cls<span style=\"color: #AA22FF; font-weight: bold\">.</span>fused_relax_multiply(\n",
       "                lv1, lv_1\n",
       "            )\n",
       "            gv: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> cls<span style=\"color: #AA22FF; font-weight: bold\">.</span>fused_relax_add(lv1_1, lv)\n",
       "            R<span style=\"color: #AA22FF; font-weight: bold\">.</span>output(gv)\n",
       "        <span style=\"color: #008000; font-weight: bold\">return</span> gv\n",
       "</pre></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tvm import relax\n",
    "mod1 = relax.transform.FuseOpsByPattern(patterns)(mod)\n",
    "mod1.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, `FuseOpsByPattern` splits a graph into a set of composite functions based on the given list of patterns and appends assigned names in the function attribute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #007979; font-style: italic\"># from tvm.script import ir as I</span>\n",
       "<span style=\"color: #007979; font-style: italic\"># from tvm.script import relax as R</span>\n",
       "\n",
       "\n",
       "<span style=\"color: #AA22FF\">@I</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>ir_module\n",
       "<span style=\"color: #008000; font-weight: bold\">class</span> <span style=\"color: #0000FF; font-weight: bold\">Module</span>:\n",
       "    <span style=\"color: #AA22FF\">@R</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>function\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">fused_relax_multiply_relax_add_relax_add_relax_multiply_relax_add</span>(\n",
       "        x: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), y: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)\n",
       "    ) <span style=\"color: #AA22FF; font-weight: bold\">-&gt;</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>):\n",
       "        R<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr(\n",
       "            {\n",
       "                <span style=\"color: #BA2121\">&quot;Codegen&quot;</span>: <span style=\"color: #BA2121\">&quot;tensorrt&quot;</span>,\n",
       "                <span style=\"color: #BA2121\">&quot;Primitive&quot;</span>: <span style=\"color: #008000\">1</span>,\n",
       "                <span style=\"color: #BA2121\">&quot;global_symbol&quot;</span>: <span style=\"color: #BA2121\">&quot;fused_relax_multiply_relax_add_relax_add_relax_multiply_relax_add&quot;</span>,\n",
       "            }\n",
       "        )\n",
       "        <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>dataflow():\n",
       "            <span style=\"color: #007979; font-style: italic\"># from tvm.script import relax as R</span>\n",
       "\n",
       "            <span style=\"color: #AA22FF\">@R</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>function\n",
       "            <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">lv_1</span>(\n",
       "                x_1: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>),\n",
       "                y_1: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>),\n",
       "            ) <span style=\"color: #AA22FF; font-weight: bold\">-&gt;</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>):\n",
       "                R<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;Composite&quot;</span>: <span style=\"color: #BA2121\">&quot;tensorrt.multiply&quot;</span>, <span style=\"color: #BA2121\">&quot;Primitive&quot;</span>: <span style=\"color: #008000\">1</span>})\n",
       "                <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>dataflow():\n",
       "                    gv: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>multiply(x_1, y_1)\n",
       "                    R<span style=\"color: #AA22FF; font-weight: bold\">.</span>output(gv)\n",
       "                <span style=\"color: #008000; font-weight: bold\">return</span> gv\n",
       "\n",
       "            lv_1: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> lv(x, y)\n",
       "            <span style=\"color: #007979; font-style: italic\"># from tvm.script import relax as R</span>\n",
       "\n",
       "            <span style=\"color: #AA22FF\">@R</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>function\n",
       "            <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">lv1_1</span>(\n",
       "                lv_2: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>),\n",
       "                x_1: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>),\n",
       "            ) <span style=\"color: #AA22FF; font-weight: bold\">-&gt;</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>):\n",
       "                R<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;Composite&quot;</span>: <span style=\"color: #BA2121\">&quot;tensorrt.add&quot;</span>, <span style=\"color: #BA2121\">&quot;Primitive&quot;</span>: <span style=\"color: #008000\">1</span>})\n",
       "                <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>dataflow():\n",
       "                    gv: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>add(lv_2, x_1)\n",
       "                    R<span style=\"color: #AA22FF; font-weight: bold\">.</span>output(gv)\n",
       "                <span style=\"color: #008000; font-weight: bold\">return</span> gv\n",
       "\n",
       "            lv_2: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> lv1(lv_1, x)\n",
       "            lv1_1: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> lv1(lv_1, lv_2)\n",
       "            lv1_2: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> lv(lv1_1, lv_2)\n",
       "            gv: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> lv1(lv1_2, lv_1)\n",
       "            R<span style=\"color: #AA22FF; font-weight: bold\">.</span>output(gv)\n",
       "        <span style=\"color: #008000; font-weight: bold\">return</span> gv\n",
       "\n",
       "    <span style=\"color: #AA22FF\">@R</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>function\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">main</span>(\n",
       "        x: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), y: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)\n",
       "    ) <span style=\"color: #AA22FF; font-weight: bold\">-&gt;</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>):\n",
       "        cls <span style=\"color: #AA22FF; font-weight: bold\">=</span> Module\n",
       "        <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>dataflow():\n",
       "            gv: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor(\n",
       "                (<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>\n",
       "            ) <span style=\"color: #AA22FF; font-weight: bold\">=</span> cls<span style=\"color: #AA22FF; font-weight: bold\">.</span>fused_relax_multiply_relax_add_relax_add_relax_multiply_relax_add(\n",
       "                x, y\n",
       "            )\n",
       "            R<span style=\"color: #AA22FF; font-weight: bold\">.</span>output(gv)\n",
       "        <span style=\"color: #008000; font-weight: bold\">return</span> gv\n",
       "</pre></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mod2 = relax.transform.MergeCompositeFunctions()(mod1)\n",
    "mod2.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`MergeCompositeFunctions` combines adjacent composite functions. Also, this pass annotates target codegen and global symbol that will be used by the following `RunCodegen`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #007979; font-style: italic\"># from tvm.script import ir as I</span>\n",
       "<span style=\"color: #007979; font-style: italic\"># from tvm.script import relax as R</span>\n",
       "\n",
       "\n",
       "<span style=\"color: #AA22FF\">@I</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>ir_module\n",
       "<span style=\"color: #008000; font-weight: bold\">class</span> <span style=\"color: #0000FF; font-weight: bold\">Module</span>:\n",
       "    <span style=\"color: #AA22FF\">@R</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>function\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">main</span>(\n",
       "        x: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), y: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)\n",
       "    ) <span style=\"color: #AA22FF; font-weight: bold\">-&gt;</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>):\n",
       "        <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>dataflow():\n",
       "            gv <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>call_dps_packed(\n",
       "                <span style=\"color: #BA2121\">&quot;fused_relax_multiply_relax_add_relax_add_relax_multiply_relax_add&quot;</span>,\n",
       "                (x, y),\n",
       "                out_sinfo<span style=\"color: #AA22FF; font-weight: bold\">=</span>R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>),\n",
       "            )\n",
       "            R<span style=\"color: #AA22FF; font-weight: bold\">.</span>output(gv)\n",
       "        <span style=\"color: #008000; font-weight: bold\">return</span> gv\n",
       "</pre></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorRT runtime module: [runtime.Module(0x4d951d8)]\n"
     ]
    }
   ],
   "source": [
    "mod3 = relax.transform.RunCodegen()(mod2)\n",
    "mod3.show()\n",
    "# Produced runtime module will be attached in the IRModule attribute.\n",
    "print(f\"TensorRT runtime module: {mod3.attrs['external_mods']}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`RunCodegen` converts composite functions of interest into the external runtime module by using each codegen. Then, instead of calling into Relax composite functions, we will invoke this BYOC runtime module attached in the IRModule attribute. \n",
    "\n",
    "(4) Now we are ready to run the model. Check if the final IRModule is well-formed, build and run it with Relax virtual machine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10:04:19] /home/spark/tvm/src/runtime/contrib/tensorrt/tensorrt_logger.h:54: Warning: WARNING: Tensor DataType is determined at build time for tensors not marked as input or output.\n",
      "[10:04:19] /home/spark/tvm/src/runtime/contrib/tensorrt/tensorrt_logger.h:54: Warning: WARNING: Tensor DataType is determined at build time for tensors not marked as input or output.\n",
      "[10:04:19] /home/spark/tvm/src/runtime/contrib/tensorrt/tensorrt_logger.h:54: Warning: WARNING: Tensor DataType is determined at build time for tensors not marked as input or output.\n",
      "[10:04:19] /home/spark/tvm/src/runtime/contrib/tensorrt/tensorrt_logger.h:54: Warning: WARNING: Tensor DataType is determined at build time for tensors not marked as input or output.\n",
      "[10:04:19] /home/spark/tvm/src/runtime/contrib/tensorrt/tensorrt_logger.h:54: Warning: WARNING: Tensor DataType is determined at build time for tensors not marked as input or output.\n",
      "[10:04:20] /home/spark/tvm/src/runtime/contrib/tensorrt/tensorrt_logger.h:54: Warning: WARNING: TensorRT was linked against cuBLAS/cuBLAS LT 11.6.5 but loaded cuBLAS/cuBLAS LT 11.3.1\n",
      "[10:04:20] /home/spark/tvm/src/runtime/contrib/tensorrt/tensorrt_logger.h:54: Warning: WARNING: TensorRT was linked against cuDNN 8.2.1 but loaded cuDNN 8.1.1\n",
      "[10:04:21] /home/spark/tvm/src/runtime/contrib/tensorrt/tensorrt_logger.h:54: Warning: WARNING: TensorRT was linked against cuBLAS/cuBLAS LT 11.6.5 but loaded cuBLAS/cuBLAS LT 11.3.1\n",
      "[10:04:21] /home/spark/tvm/src/runtime/contrib/tensorrt/tensorrt_logger.h:54: Warning: WARNING: TensorRT was linked against cuDNN 8.2.1 but loaded cuDNN 8.1.1\n",
      "[10:04:21] /home/spark/tvm/src/runtime/contrib/tensorrt/tensorrt_logger.h:54: Warning: WARNING: TensorRT was linked against cuBLAS/cuBLAS LT 11.6.5 but loaded cuBLAS/cuBLAS LT 11.3.1\n",
      "[10:04:21] /home/spark/tvm/src/runtime/contrib/tensorrt/tensorrt_logger.h:54: Warning: WARNING: TensorRT was linked against cuDNN 8.2.1 but loaded cuDNN 8.1.1\n"
     ]
    }
   ],
   "source": [
    "# Check if output IRModule is well-formed. \n",
    "assert relax.analysis.well_formed(mod3)\n",
    "\n",
    "# Define your target hardware and device.\n",
    "target, dev = tvm.target.Target(\"cuda\"), tvm.cuda()\n",
    "\n",
    "# Prepare inputs.\n",
    "np0 = np.random.rand(16, 16).astype(np.float32)\n",
    "np1 = np.random.rand(16, 16).astype(np.float32)\n",
    "data0 = tvm.nd.array(np0, dev)\n",
    "data1 = tvm.nd.array(np1, dev)\n",
    "inputs = [data0, data1]\n",
    "\n",
    "# Prepare expected output.\n",
    "t1 = np.multiply(np0, np1)\n",
    "t2 = np.add(t1, np0)\n",
    "t3 = np.add(t1, t2)\n",
    "t4 = np.multiply(t3, t2)\n",
    "expected = np.add(t4, t1)\n",
    "\n",
    "# Build and prepare VM. \n",
    "ex = relax.build(mod3, target, params={})\n",
    "vm = relax.VirtualMachine(ex, dev)\n",
    "\n",
    "# Run VM. \n",
    "out = vm[\"main\"](*inputs)\n",
    "\n",
    "import tvm.testing\n",
    "tvm.testing.assert_allclose(out.numpy(), expected, rtol=1e-6, atol=1e-6)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mix-and-Match BYOC and Tuning\n",
    "In Relax, you can optimize one part of the graph with BYOC while tuning other parts in a flexible way. \n",
    "In this section, from the previous example, let's say we want to offload only `add` to TensorRT and optimize `matmul` kernel with MetaSchedul tuning. \n",
    "\n",
    "Like in previous section, we define the operator patterns that we want to offload to BYOC. \n",
    "Here, we only target `add` operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [\n",
    "        (\"tensorrt.add\", is_op(\"relax.add\")(wildcard(), wildcard())),\n",
    "    ]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For MetaSchedule, you need to provide more specific hardware information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your target hardware and device.\n",
    "target, dev = tvm.target.Target(\"nvidia/geforce-rtx-3070\"), tvm.cuda()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to apply the pass sequence. On the top of the passes for BYOC offloading, we simply add additional passes for lowering and MetaSchedule tuning. \n",
    "Once BYOC passes offload the target operators (`add` in this example) to tensorrt, `LegalizeOps` pass will lower the rest of operators (`multiply` in this example) to TIR PrimFunc. Then, `MetaScheduleTuneIRMod` and `MetaScheduleApplyDatabase` will perform tuning and apply the best optimization decision based on the tuning database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-20 10:04:42 [INFO] [task_scheduler.cc:260] Task #0 has finished. Remaining task(s): 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>FLOP</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Speed (GFLOPS)</th>\n",
       "      <th>Latency (us)</th>\n",
       "      <th>Weighted Latency (us)</th>\n",
       "      <th>Trials</th>\n",
       "      <th>Done</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>multiply</td>\n",
       "      <td>256</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1619</td>\n",
       "      <td>1.5815</td>\n",
       "      <td>3.1630</td>\n",
       "      <td>4</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Name    FLOP    Weight    Speed (GFLOPS)    Latency (us)   \\\n",
       "0   multiply     256         2            0.1619          1.5815    \n",
       "\n",
       "    Weighted Latency (us)    Trials    Done   \n",
       "0                  3.1630         4       Y   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total trials: 4\n",
      "Total latency (us): 3.16303\n",
      "\n",
      "2023-03-20 10:04:42 [DEBUG] [task_scheduler.cc:318] \n",
      " ID |     Name | FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Done \n",
      "-------------------------------------------------------------------------------------------------------\n",
      "  0 | multiply |  256 |      2 |         0.1619 |       1.5815 |                3.1630 |      4 |    Y \n",
      "-------------------------------------------------------------------------------------------------------\n",
      "Total trials: 4\n",
      "Total latency (us): 3.16303\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10:04:42] /home/spark/tvm/src/relax/transform/meta_schedule.cc:102: Warning: Creating JSONDatabase. Workload at: /tmp/tmpzy2rbpc2/database_workload.json, Tuning records at: /tmp/tmpzy2rbpc2/database_tuning_record.json\n",
      "[10:04:43] /home/spark/tvm/src/runtime/contrib/tensorrt/tensorrt_logger.h:54: Warning: WARNING: Tensor DataType is determined at build time for tensors not marked as input or output.\n",
      "[10:04:43] /home/spark/tvm/src/runtime/contrib/tensorrt/tensorrt_logger.h:54: Warning: WARNING: Tensor DataType is determined at build time for tensors not marked as input or output.\n",
      "[10:04:43] /home/spark/tvm/src/runtime/contrib/tensorrt/tensorrt_logger.h:54: Warning: WARNING: TensorRT was linked against cuBLAS/cuBLAS LT 11.6.5 but loaded cuBLAS/cuBLAS LT 11.3.1\n",
      "[10:04:43] /home/spark/tvm/src/runtime/contrib/tensorrt/tensorrt_logger.h:54: Warning: WARNING: TensorRT was linked against cuDNN 8.2.1 but loaded cuDNN 8.1.1\n",
      "[10:04:43] /home/spark/tvm/src/runtime/contrib/tensorrt/tensorrt_logger.h:54: Warning: WARNING: TensorRT was linked against cuBLAS/cuBLAS LT 11.6.5 but loaded cuBLAS/cuBLAS LT 11.3.1\n",
      "[10:04:43] /home/spark/tvm/src/runtime/contrib/tensorrt/tensorrt_logger.h:54: Warning: WARNING: TensorRT was linked against cuDNN 8.2.1 but loaded cuDNN 8.1.1\n",
      "[10:04:43] /home/spark/tvm/src/runtime/contrib/tensorrt/tensorrt_logger.h:54: Warning: WARNING: TensorRT was linked against cuBLAS/cuBLAS LT 11.6.5 but loaded cuBLAS/cuBLAS LT 11.3.1\n",
      "[10:04:43] /home/spark/tvm/src/runtime/contrib/tensorrt/tensorrt_logger.h:54: Warning: WARNING: TensorRT was linked against cuDNN 8.2.1 but loaded cuDNN 8.1.1\n",
      "[10:04:43] /home/spark/tvm/src/runtime/contrib/tensorrt/tensorrt_logger.h:54: Warning: WARNING: Tensor DataType is determined at build time for tensors not marked as input or output.\n",
      "[10:04:43] /home/spark/tvm/src/runtime/contrib/tensorrt/tensorrt_logger.h:54: Warning: WARNING: TensorRT was linked against cuBLAS/cuBLAS LT 11.6.5 but loaded cuBLAS/cuBLAS LT 11.3.1\n",
      "[10:04:43] /home/spark/tvm/src/runtime/contrib/tensorrt/tensorrt_logger.h:54: Warning: WARNING: TensorRT was linked against cuDNN 8.2.1 but loaded cuDNN 8.1.1\n",
      "[10:04:43] /home/spark/tvm/src/runtime/contrib/tensorrt/tensorrt_logger.h:54: Warning: WARNING: TensorRT was linked against cuBLAS/cuBLAS LT 11.6.5 but loaded cuBLAS/cuBLAS LT 11.3.1\n",
      "[10:04:43] /home/spark/tvm/src/runtime/contrib/tensorrt/tensorrt_logger.h:54: Warning: WARNING: TensorRT was linked against cuDNN 8.2.1 but loaded cuDNN 8.1.1\n",
      "[10:04:43] /home/spark/tvm/src/runtime/contrib/tensorrt/tensorrt_logger.h:54: Warning: WARNING: TensorRT was linked against cuBLAS/cuBLAS LT 11.6.5 but loaded cuBLAS/cuBLAS LT 11.3.1\n",
      "[10:04:43] /home/spark/tvm/src/runtime/contrib/tensorrt/tensorrt_logger.h:54: Warning: WARNING: TensorRT was linked against cuDNN 8.2.1 but loaded cuDNN 8.1.1\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "from tvm.relax.transform.tuning_api import Trace\n",
    "\n",
    "# Run Codegen pass\n",
    "with tempfile.TemporaryDirectory() as work_dir:\n",
    "  with target, tvm.transform.PassContext(trace=Trace(mod)):\n",
    "      mod4 = tvm.transform.Sequential(\n",
    "        [\n",
    "                relax.transform.FuseOpsByPattern(patterns),\n",
    "                relax.transform.MergeCompositeFunctions(),\n",
    "                relax.transform.RunCodegen(),\n",
    "                relax.transform.LegalizeOps(),\n",
    "                relax.transform.MetaScheduleTuneIRMod(\n",
    "                params={}, work_dir=work_dir, max_trials_global=8\n",
    "                ),\n",
    "                relax.transform.MetaScheduleApplyDatabase(work_dir),\n",
    "        ]\n",
    "        )(mod)\n",
    "assert relax.analysis.well_formed(mod4)\n",
    "# Build and prepare VM. \n",
    "ex = relax.build(mod4, target, params={})\n",
    "vm = relax.VirtualMachine(ex, dev)\n",
    "\n",
    "# Run VM. \n",
    "out = vm[\"main\"](*inputs)\n",
    "tvm.testing.assert_allclose(out.numpy(), expected, rtol=1e-6, atol=1e-6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "relax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2901801719f5be8873801974a0e66ed2e883059183c2266065b5990cd4baee62"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
